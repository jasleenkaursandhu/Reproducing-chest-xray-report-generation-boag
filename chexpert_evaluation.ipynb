{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasleenkaursandhu/Reproducing-chest-xray-report-generation-boag/blob/main/chexpert_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-10T22:55:02.823081Z",
          "start_time": "2025-04-10T22:55:02.818098Z"
        },
        "id": "rZ-dMbySGkBB"
      },
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "# base_path = '/Volumes/DATA/DATASET/untitled/content/mimic-cxr-project/' # For local machine\n",
        "# base_path = '/content/drive/MyDrive/mimic-cxr-project' # For Google Colab"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and mount Google Drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PySNzwam6ss-",
        "outputId": "6732fe19-d9a9-4557-c070-82bdd3b5122d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/mimic-cxr-project'\n",
        "output_dir = os.path.join(base_path, 'output')\n",
        "\n",
        "# Print available files to verify\n",
        "print(\"Files in output directory:\")\n",
        "print(os.listdir(output_dir))\n",
        "\n",
        "# Load labeled files\n",
        "reference_df = pd.read_csv(os.path.join(output_dir, 'labeled_reference.csv'))\n",
        "random_df = pd.read_csv(os.path.join(output_dir, 'labeled_random.csv'))\n",
        "ngram_df = pd.read_csv(os.path.join(output_dir, 'labeled_3gram.csv'))\n",
        "\n",
        "print(f\"\\nReference shape: {reference_df.shape}\")\n",
        "print(f\"Random shape: {random_df.shape}\")\n",
        "print(f\"N-gram shape: {ngram_df.shape}\")\n",
        "\n",
        "# Get all the categories (columns)\n",
        "categories = reference_df.columns.tolist()\n",
        "print(f\"\\nCategories: {categories}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azh-pRUX63h1",
        "outputId": "3d04d536-5d22-40d2-cb0e-8b1a0446c164"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in output directory:\n",
            "['reference_input.csv', 'random_input.csv', '.ipynb_checkpoints', 'densenet121_train.pkl', 'densenet121_test.pkl', 'top100_neighbors.pkl', 'cider_comparison.png', 'bleu_comparison.png', 'ngram_bleu_comparison.png', 'ngram_cider_comparison.png', 'reference.tsv', 'reference_chexpert_format.csv', 'labeled_reference.csv', 'labeled_3gram.csv', 'labeled_random.csv', 'random.tsv', '3-gram.tsv', 'knn.tsv', 'knn_bleu_comparison.png', 'knn_cider_comparison.png']\n",
            "\n",
            "Reference shape: (940, 15)\n",
            "Random shape: (382, 15)\n",
            "N-gram shape: (380, 15)\n",
            "\n",
            "Categories: ['Reports', 'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Lesion', 'Lung Opacity', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-10T22:55:02.853091Z",
          "start_time": "2025-04-10T22:55:02.836738Z"
        },
        "id": "4u9lpvvIGkBD",
        "outputId": "a5188fc2-286c-485c-e480-dc42e1d0cb56"
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from time import strftime, gmtime\n",
        "import os\n",
        "\n",
        "# Define the base path\n",
        "base_path = '/Volumes/DATA/DATASET/untitled/content/mimic-cxr-project/'\n",
        "\n",
        "# input file\n",
        "input_file = os.path.join(base_path, \"output\", \"reference.csv\")\n",
        "\n",
        "#  output file\n",
        "output_file = os.path.join(base_path, \"output\", \"reference_headerless.csv\")\n",
        "\n",
        "# Create a dictionary to store the reports\n",
        "ref_reports = {}\n",
        "\n",
        "# Read the input file\n",
        "with open(input_file, 'r') as f:\n",
        "    # Check if there's a header by reading the first line\n",
        "    first_line = f.readline().strip()\n",
        "    has_header = 'dicom_id' in first_line and '\\t' in first_line\n",
        "\n",
        "    # If there's a header, we've already consumed it, if not we need to process the line\n",
        "    if not has_header:\n",
        "        # Process the first line as it contains data\n",
        "        parts = first_line.split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            dicom_id = parts[0]\n",
        "            text = '\\t'.join(parts[1:])  # In case there are multiple tabs in the text\n",
        "            ref_reports[dicom_id] = text\n",
        "\n",
        "    # Process the rest of the lines\n",
        "    for line in f:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            dicom_id = parts[0]\n",
        "            text = '\\t'.join(parts[1:])  # In case there are multiple tabs in the text\n",
        "            ref_reports[dicom_id] = text\n",
        "\n",
        "# Now write the reports to the output file in the required format\n",
        "with open(output_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
        "    # No header (headerless as required)\n",
        "\n",
        "    for dicom_id, text in sorted(ref_reports.items()):\n",
        "        # Only include the text as a single column\n",
        "        writer.writerow([text])\n",
        "\n",
        "print(f\"Processed {len(ref_reports)} reports\")\n",
        "print(f\"Output written to {output_file}\")\n",
        "print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 940 reports\n",
            "Output written to /Volumes/DATA/DATASET/untitled/content/mimic-cxr-project/output/reference_headerless.csv\n",
            "2025-04-10 22:55:02\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "aCnQm_vdrooM",
        "ExecuteTime": {
          "end_time": "2025-04-10T23:06:28.253004Z",
          "start_time": "2025-04-10T22:55:02.923239Z"
        },
        "outputId": "a7171223-18ea-404f-eb36-7c90184ab07f"
      },
      "cell_type": "code",
      "source": [
        "docker_image_name = 'uwizeye2/chexpert-labeler:amd64'\n",
        "!docker run --platform linux/amd64 -v {base_path}output:/data {docker_image_name} python label.py --reports_path /data/reference_headerless.csv --output_path /data/labeled_reference.csv --verbose"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 940/940 [00:00<00:00, 1008.31it/s]\r\n",
            " 53%|█████▎    | 500/940 [05:29<09:31,  1.30s/it]ERROR:root:No parse tree for sentence: 0\r\n",
            "NoneType: None\r\n",
            " 62%|██████▏   | 586/940 [06:37<03:08,  1.88it/s]ERROR:root:Cannot process sentence 152 in 586\r\n",
            "Traceback (most recent call last):\r\n",
            "  File \"/app/chexpert-labeler/NegBio/negbio/pipeline/ptb2ud.py\", line 120, in convert_doc\r\n",
            "    has_lemmas=self._backend == 'jpype')\r\n",
            "TypeError: 'NoneType' object is not iterable\r\n",
            " 91%|█████████▏| 859/940 [10:11<01:03,  1.28it/s]ERROR:root:Cannot process sentence 31 in 859\r\n",
            "Traceback (most recent call last):\r\n",
            "  File \"/app/chexpert-labeler/NegBio/negbio/pipeline/ptb2ud.py\", line 120, in convert_doc\r\n",
            "    has_lemmas=self._backend == 'jpype')\r\n",
            "TypeError: 'NoneType' object is not iterable\r\n",
            "100%|██████████| 940/940 [11:08<00:00,  1.57it/s]\r\n",
            "100%|██████████| 940/940 [00:00<00:00, 71794.12it/s]\r\n",
            "Loading mention phrases for 14 observations.\r\n",
            "Loading unmention phrases for 3 observations.\r\n",
            "Extracting mentions...\r\n",
            "Classifying mentions...\r\n",
            "Aggregating mentions...\r\n",
            "Writing reports and labels to /data/labeled_reference.csv.\r\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation function with sample matching\n",
        "def calculate_f1(true_df, pred_df):\n",
        "    # Find common indices\n",
        "    common_indices = true_df.index.intersection(pred_df.index)\n",
        "    if len(common_indices) == 0:\n",
        "        # If no common indices found by index, try matching by report text if available\n",
        "        if 'Reports' in true_df.columns and 'Reports' in pred_df.columns:\n",
        "            # Create dictionaries mapping reports to their rows\n",
        "            true_reports = {report: i for i, report in enumerate(true_df['Reports'].values)}\n",
        "            pred_reports = {report: i for i, report in enumerate(pred_df['Reports'].values)}\n",
        "            # Find common reports\n",
        "            common_reports = set(true_reports.keys()).intersection(set(pred_reports.keys()))\n",
        "            if common_reports:\n",
        "                print(f\"Found {len(common_reports)} common reports by text matching\")\n",
        "                # Extract rows with common reports\n",
        "                true_subset = true_df.loc[[true_reports[report] for report in common_reports]]\n",
        "                pred_subset = pred_df.loc[[pred_reports[report] for report in common_reports]]\n",
        "            else:\n",
        "                # No matching by text either, try using the first rows\n",
        "                print(\"No common samples found. Using first min(len1, len2) rows.\")\n",
        "                min_rows = min(len(true_df), len(pred_df))\n",
        "                true_subset = true_df.iloc[:min_rows]\n",
        "                pred_subset = pred_df.iloc[:min_rows]\n",
        "        else:\n",
        "            # No reports column, try matching row by row\n",
        "            print(\"No common indices found. Using first min(len1, len2) rows.\")\n",
        "            min_rows = min(len(true_df), len(pred_df))\n",
        "            true_subset = true_df.iloc[:min_rows]\n",
        "            pred_subset = pred_df.iloc[:min_rows]\n",
        "    else:\n",
        "        print(f\"Found {len(common_indices)} common indices\")\n",
        "        true_subset = true_df.loc[common_indices]\n",
        "        pred_subset = pred_df.loc[common_indices]\n",
        "\n",
        "    # Ensure all categories exist in both dataframes\n",
        "    categories = [col for col in true_subset.columns if col in pred_subset.columns and col != 'Reports']\n",
        "\n",
        "    # Fill missing values with -2 (not mentioned)\n",
        "    true_subset = true_subset[categories].fillna(-2)\n",
        "    pred_subset = pred_subset[categories].fillna(-2)\n",
        "\n",
        "    # For F1 score, only positive mentions (1) count as positive\n",
        "    true_binary = (true_subset == 1).astype(int)\n",
        "    pred_binary = (pred_subset == 1).astype(int)\n",
        "\n",
        "    # Calculate F1 per category\n",
        "    f1_scores = {}\n",
        "    for category in categories:\n",
        "        f1 = f1_score(true_binary[category], pred_binary[category], zero_division=0)\n",
        "        f1_scores[category] = f1\n",
        "\n",
        "    # Add macro average\n",
        "    macro_f1 = np.mean(list(f1_scores.values()))\n",
        "    f1_scores['Macro Average'] = macro_f1\n",
        "\n",
        "    return f1_scores"
      ],
      "metadata": {
        "id": "vcgUexq96-ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Random Model\n",
        "print(\"Evaluating Random Model...\")\n",
        "random_f1_scores = calculate_f1(reference_df, random_df)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nRandom model F1 scores:\")\n",
        "for category, score in random_f1_scores.items():\n",
        "    print(f\"{category}: {score:.4f}\")\n",
        "\n",
        "# Paper's reported score\n",
        "paper_random_f1 = 0.148\n",
        "print(f\"\\nComparison with paper:\")\n",
        "print(f\"Random model macro-F1: Ours = {random_f1_scores['Macro Average']:.4f}, Paper = {paper_random_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZsUDnFY7BTF",
        "outputId": "36326063-f72a-401c-b7bd-643901b319ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Random Model...\n",
            "Found 382 common indices\n",
            "\n",
            "Random model F1 scores:\n",
            "No Finding: 0.2446\n",
            "Enlarged Cardiomediastinum: 0.1124\n",
            "Cardiomegaly: 0.3016\n",
            "Lung Lesion: 0.1000\n",
            "Lung Opacity: 0.4294\n",
            "Edema: 0.1250\n",
            "Consolidation: 0.0000\n",
            "Pneumonia: 0.0000\n",
            "Atelectasis: 0.2700\n",
            "Pneumothorax: 0.0000\n",
            "Pleural Effusion: 0.3448\n",
            "Pleural Other: 0.0000\n",
            "Fracture: 0.1081\n",
            "Support Devices: 0.5160\n",
            "Macro Average: 0.1823\n",
            "\n",
            "Comparison with paper:\n",
            "Random model macro-F1: Ours = 0.1823, Paper = 0.1480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate N-gram Model\n",
        "print(\"Evaluating N-gram Model...\")\n",
        "ngram_f1_scores = calculate_f1(reference_df, ngram_df)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nN-gram model F1 scores:\")\n",
        "for category, score in ngram_f1_scores.items():\n",
        "    print(f\"{category}: {score:.4f}\")\n",
        "\n",
        "# Paper's reported score\n",
        "paper_ngram_f1 = 0.185\n",
        "print(f\"\\nComparison with paper:\")\n",
        "print(f\"N-gram model macro-F1: Ours = {ngram_f1_scores['Macro Average']:.4f}, Paper = {paper_ngram_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTYeqal47efq",
        "outputId": "f626ea90-f72d-4514-fdc9-877467613279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating N-gram Model...\n",
            "Found 380 common indices\n",
            "\n",
            "N-gram model F1 scores:\n",
            "No Finding: 0.1455\n",
            "Enlarged Cardiomediastinum: 0.0870\n",
            "Cardiomegaly: 0.3217\n",
            "Lung Lesion: 0.0000\n",
            "Lung Opacity: 0.3295\n",
            "Edema: 0.1818\n",
            "Consolidation: 0.0444\n",
            "Pneumonia: 0.0000\n",
            "Atelectasis: 0.2485\n",
            "Pneumothorax: 0.0476\n",
            "Pleural Effusion: 0.2116\n",
            "Pleural Other: 0.2222\n",
            "Fracture: 0.0000\n",
            "Support Devices: 0.4556\n",
            "Macro Average: 0.1640\n",
            "\n",
            "Comparison with paper:\n",
            "N-gram model macro-F1: Ours = 0.1640, Paper = 0.1850\n"
          ]
        }
      ]
    }
  ]
}