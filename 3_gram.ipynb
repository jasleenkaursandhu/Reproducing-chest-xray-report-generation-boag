{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCfsL5bYIgDZvNw0DFWVez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasleenkaursandhu/Reproducing-chest-xray-report-generation-boag/blob/main/3_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP0BluOO0DY1"
      },
      "outputs": [],
      "source": [
        "# N-gram Model for Report Generation\n",
        "# This notebook implements an 3-gram language model for chest X-ray report generation.\n",
        "# The model selects the most similar training images for each test image and builds a language model from their reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "import pickle\n",
        "import gzip\n",
        "import random\n",
        "import re\n",
        "import warnings\n",
        "!pip install pydicom\n",
        "import pydicom\n",
        "from time import gmtime, strftime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOX8yrJv0LFn",
        "outputId": "62ec7394-eb33-479a-9bbb-40db10d02012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/mimic-cxr-project'\n",
        "!mkdir -p {base_path}/data\n",
        "!mkdir -p {base_path}/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WANs5gfU0Rki",
        "outputId": "432270ad-01f9-46e5-93ac-3790064d8144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the report parser module\n",
        "import sys\n",
        "sys.path.append(f\"{base_path}/modules\")\n",
        "from report_parser import parse_report, MIMIC_RE\n",
        "print(\"Successfully imported report parser module\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Mq-yU00XtS",
        "outputId": "f64ea13c-c064-441e-e93d-de2589dcf3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully imported report parser module\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test data\n",
        "data_dir = os.path.join(base_path, 'data')\n",
        "files_path = os.path.join(base_path, 'files')\n",
        "output_dir = os.path.join(base_path, 'output')\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t')\n",
        "test_df = pd.read_csv(os.path.join(data_dir, 'test.tsv'), sep='\\t')\n",
        "\n",
        "print(f\"Train data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZKWVShS0bVI",
        "outputId": "d4412fc2-7c08-4a08-a686-021d79e624e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (824, 3)\n",
            "Test data shape: (382, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the top 100 neighbors for each test image\n",
        "neighbors_path = os.path.join(output_dir, 'top100_neighbors.pkl')\n",
        "\n",
        "if os.path.exists(neighbors_path):\n",
        "    with open(neighbors_path, 'rb') as f:\n",
        "        neighbors = pickle.load(f)\n",
        "\n",
        "    print(f\"Loaded neighbors for {len(neighbors)} test images\")\n",
        "    print(f\"Sample neighbors for first test image: {list(neighbors.items())[0][1][:5]}...\")\n",
        "else:\n",
        "    print(f\"Warning: Neighbors file not found at {neighbors_path}\")\n",
        "    print(\"Please run the KNN model first to generate the neighbors file.\")\n",
        "    neighbors = {}\n",
        "    for dicom_id in test_df.dicom_id.values:\n",
        "        neighbors[dicom_id] = random.sample(train_df.dicom_id.tolist(), min(100, len(train_df)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wplhnyXG0fja",
        "outputId": "7aa187a1-e460-4efe-98df-5354a7ca3b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded neighbors for 380 test images\n",
            "Sample neighbors for first test image: ['15a0e62a-ac8edf75-3444949e-35cf275c-b22ee616', '1d00cfa1-29d99da7-c62126a2-18449dbb-6dd404f0', 'fd228853-2df84977-18361e36-f22ccc25-7d9a4046', '350acbc7-85b7cb9f-030eeac1-1e4ff930-a29191a1', '76bdc346-c4561bc4-c75ab157-4fdcde0e-843596ec']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map each dicom to its study_id\n",
        "report_id_column = 'study_id'\n",
        "report_lookup = dict(train_df[['dicom_id', report_id_column]].values)\n",
        "print(f\"Created lookup dictionary for {len(report_lookup)} training images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82hk_03j0poo",
        "outputId": "d93fd9eb-eacc-48fa-e25c-c7a36361527a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created lookup dictionary for 824 training images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokens for sequence boundaries\n",
        "START = '<START>'\n",
        "END = '<END>'\n",
        "\n",
        "# Build n-gram language model from neighbors\n",
        "def fit(dicom_ids, n=3):\n",
        "    \"\"\"Build language model from the reports of the given dicom_ids\"\"\"\n",
        "    # Language model maps context (n-1 previous words) to possible next words\n",
        "    LM = defaultdict(Counter)\n",
        "\n",
        "    for dicom_id in dicom_ids:\n",
        "        if dicom_id not in report_lookup:\n",
        "            continue\n",
        "\n",
        "        report_id = report_lookup[dicom_id]\n",
        "\n",
        "        # Get corresponding subject_id\n",
        "        subject_row = train_df[train_df.dicom_id == dicom_id]\n",
        "        if len(subject_row) == 0:\n",
        "            continue\n",
        "\n",
        "        subject_id = subject_row.iloc[0]['subject_id']\n",
        "\n",
        "        # Construct path to the report\n",
        "        subject_prefix = f\"p{str(subject_id)[:2]}\"\n",
        "        subject_dir = f\"p{subject_id}\"\n",
        "        study_dir = f\"s{report_id}\"\n",
        "        report_file = f\"{study_dir}.txt\"\n",
        "        report_path = os.path.join(files_path, subject_prefix, subject_dir, report_file)\n",
        "\n",
        "        # Parse the report\n",
        "        try:\n",
        "            if os.path.exists(report_path):\n",
        "                parsed_report = parse_report(report_path)\n",
        "\n",
        "                if 'findings' in parsed_report:\n",
        "                    # Tokenize the findings text\n",
        "                    toks = parsed_report['findings'].replace('.', ' . ').split()\n",
        "\n",
        "                    # Add padding tokens at the beginning and END token at the end\n",
        "                    padded_toks = [START for _ in range(n-1)] + toks + [END]\n",
        "\n",
        "                    # Build n-gram model by counting follow words for each context\n",
        "                    for i in range(len(padded_toks) - n + 1):\n",
        "                        context = tuple(padded_toks[i:i+n-1])\n",
        "                        target = padded_toks[i+n-1]\n",
        "                        sim = 1\n",
        "                        LM[context][target] += sim\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return LM"
      ],
      "metadata": {
        "id": "z9qIMiLI0wq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from the n-gram model\n",
        "def sample(LM, seq_so_far, n):\n",
        "    \"\"\"Sample the next word based on the n-gram language model\"\"\"\n",
        "    last = tuple(seq_so_far[-(n-1):])\n",
        "\n",
        "    if last not in LM or not LM[last]:\n",
        "        # If context not found in model, return END token\n",
        "        return END\n",
        "\n",
        "    words, counts = zip(*LM[last].items())\n",
        "    total = sum(counts)\n",
        "    P = np.array(counts) / total\n",
        "\n",
        "    # Sample next word based on probabilities\n",
        "    choice = np.random.choice(words, p=P)\n",
        "    return choice"
      ],
      "metadata": {
        "id": "f_f30cQv01P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set n-gram size\n",
        "n = 3\n",
        "\n",
        "# Generate reports for test images\n",
        "generated_reports = {}\n",
        "\n",
        "print(f\"Generating reports with {n}-gram model...\")\n",
        "for pred_dicom in tqdm.tqdm(test_df.dicom_id.values):\n",
        "    # Skip if we don't have neighbors for this test image\n",
        "    if pred_dicom not in neighbors:\n",
        "        print(f\"Warning: No neighbors for {pred_dicom}\")\n",
        "        continue\n",
        "\n",
        "    # Build n-gram model from the neighbors' reports\n",
        "    nn = neighbors[pred_dicom]\n",
        "    LM = fit(nn, n=n)\n",
        "\n",
        "    # Skip if model is empty\n",
        "    if not LM:\n",
        "        print(f\"Warning: Empty language model for {pred_dicom}\")\n",
        "        continue\n",
        "\n",
        "    # Generate report by sampling from the n-gram model\n",
        "    generated_toks = [START for _ in range(n-1)]\n",
        "    current = generated_toks[-1]\n",
        "\n",
        "    # Generate until END token or max length\n",
        "    while current != END and len(generated_toks) < 100:\n",
        "        next_word = sample(LM, generated_toks, n)\n",
        "        generated_toks.append(next_word)\n",
        "        current = next_word\n",
        "\n",
        "    # Remove START tokens and potentially END token\n",
        "    generated_toks = generated_toks[n-1:]\n",
        "    if generated_toks[-1] == END:\n",
        "        generated_toks = generated_toks[:-1]\n",
        "\n",
        "    # Join tokens into text\n",
        "    generated_text = ' '.join(generated_toks)\n",
        "    generated_reports[pred_dicom] = generated_text\n",
        "\n",
        "print(f\"Generated reports for {len(generated_reports)} test images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXx2HKrD055T",
        "outputId": "a0784be2-48bb-4e5d-b195-77507001ac46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating reports with 3-gram model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█▏        | 43/382 [03:48<10:17,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No neighbors for 7f7346e9-c1f9639f-8e83f5bc-f166c421-69f3b162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 342/382 [07:40<00:28,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No neighbors for fdba0667-faa73efd-da3746a5-2a72a1fa-f5b292b7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 382/382 [08:12<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated reports for 380 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the generated reports\n",
        "print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "pred_dir = os.path.join(base_path, 'output')\n",
        "os.makedirs(pred_dir, exist_ok=True)\n",
        "\n",
        "# Save the generated reports\n",
        "pred_file = os.path.join(pred_dir, f'{n}-gram.tsv')\n",
        "print(f\"Saving predictions to {pred_file}\")\n",
        "\n",
        "with open(pred_file, 'w') as f:\n",
        "    print('dicom_id\\tgenerated', file=f)\n",
        "    for dicom_id, generated in sorted(generated_reports.items()):\n",
        "        # Clean up the text (remove any tabs)\n",
        "        cleaned_text = generated.replace('\\t', ' ')\n",
        "        print(f'{dicom_id}\\t{cleaned_text}', file=f)\n",
        "\n",
        "print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7acpffo09p2",
        "outputId": "9ffc3259-c814-43ea-ad55-722dc506224b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-13 20:38:31\n",
            "Saving predictions to /content/drive/MyDrive/mimic-cxr-project/output/3-gram.tsv\n",
            "2025-04-13 20:38:31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample of generated reports\n",
        "sample_count = min(3, len(generated_reports))\n",
        "sample_dicoms = list(generated_reports.keys())[:sample_count]\n",
        "\n",
        "for dicom_id in sample_dicoms:\n",
        "    print(f\"\\nSample report for {dicom_id}:\")\n",
        "    report_text = generated_reports[dicom_id]\n",
        "\n",
        "    # Print preview of the report (first 200 characters)\n",
        "    if len(report_text) > 200:\n",
        "        print(report_text[:200] + \"...\")\n",
        "    else:\n",
        "        print(report_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhBR88bj1Ai3",
        "outputId": "c44ae919-abda-42d1-b3e4-8bb4427568ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample report for fea90b5d-a059ecc8-b5e68f8d-b7f33ed9-1d32d429:\n",
            "compared with the tip in the left lung base is unchanged . the pulmonary vasculature is not enlarged . the costophrenic angles are not included . there is no focal consolidation, pleural effusion, pne...\n",
            "\n",
            "Sample report for 06ba097a-bf8b917c-851d0e95-886936a1-90964781:\n",
            "supine portable ap chest radiograph . the costophrenic angles are not included . there continues to be large . sternal wires are intact .\n",
            "\n",
            "Sample report for 648449fa-5e173b2a-87663d57-4a0fbfc0-138e42c7:\n",
            "left chest tube is seen . overall cardiomediastinal silhouette is enlarged . the aorta is mildly enlarged with engorgement of the cardiac silhouette continues to be some volume loss/ infiltrate in the...\n"
          ]
        }
      ]
    }
  ]
}