{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVAsxtMuMG2+bcdnf4Q4ZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasleenkaursandhu/Reproducing-chest-xray-report-generation-boag/blob/densenet121-features-nmodels/extract_densenet_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUvyFXXwk4ko",
        "outputId": "f8b719c7-9d64-4481-813b-0a685493e665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydicom\n",
        "!pip install tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pydicom\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "import pickle\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/drive/MyDrive/mimic-cxr-project'\n",
        "data_dir = os.path.join(base_path, 'data')\n",
        "files_path = os.path.join(base_path, 'files')\n",
        "output_dir = os.path.join(base_path, 'output')\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load train and test data\n",
        "train_df = pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t')\n",
        "test_df = pd.read_csv(os.path.join(data_dir, 'test.tsv'), sep='\\t')\n",
        "\n",
        "print(f\"Train data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "\n",
        "# Load pre-trained DenseNet121 model\n",
        "model = DenseNet121(weights='imagenet', include_top=False, pooling='avg')\n",
        "print(\"Loaded DenseNet121 model\")\n",
        "\n",
        "# Function to extract features from a DICOM image\n",
        "def extract_features(dicom_path):\n",
        "    try:\n",
        "        # Read the DICOM file\n",
        "        ds = pydicom.dcmread(dicom_path)\n",
        "\n",
        "        # Convert to image format\n",
        "        pixel_array = ds.pixel_array\n",
        "\n",
        "        # Normalize pixel values\n",
        "        pixel_array = pixel_array / np.max(pixel_array)\n",
        "\n",
        "        # Convert to uint8\n",
        "        img = np.uint8(pixel_array * 255)\n",
        "\n",
        "        # Convert to RGB (DenseNet expects 3 channels)\n",
        "        if len(img.shape) == 2:\n",
        "            # Grayscale to RGB\n",
        "            img_rgb = np.stack([img, img, img], axis=2)\n",
        "        elif img.shape[2] == 1:\n",
        "            # Single channel to RGB\n",
        "            img_rgb = np.concatenate([img, img, img], axis=2)\n",
        "        else:\n",
        "            img_rgb = img\n",
        "\n",
        "        # Resize to 224x224 (expected by DenseNet)\n",
        "        pil_img = Image.fromarray(img_rgb)\n",
        "        pil_img = pil_img.resize((224, 224))\n",
        "\n",
        "        # Convert to numpy array and preprocess\n",
        "        img_array = np.array(pil_img)\n",
        "        img_array = preprocess_input(img_array)\n",
        "\n",
        "        # Add batch dimension\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Extract features\n",
        "        features = model.predict(img_array, verbose=0)\n",
        "\n",
        "        return features.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {dicom_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process train images in batches to avoid memory issues\n",
        "batch_size = 50\n",
        "densenet_vecs = {}\n",
        "\n",
        "# Process train images\n",
        "for idx, row in tqdm.tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing train images\"):\n",
        "    if idx % batch_size == 0:\n",
        "        print(f\"Processed {idx}/{len(train_df)} train images\")\n",
        "\n",
        "    dicom_id = row['dicom_id']\n",
        "    subject_id = row['subject_id']\n",
        "    study_id = row['study_id']\n",
        "\n",
        "    # Construct path to the DICOM file\n",
        "    subject_prefix = f\"p{str(subject_id)[:2]}\"\n",
        "    subject_dir = f\"p{subject_id}\"\n",
        "    study_dir = f\"s{study_id}\"\n",
        "    dicom_file = f\"{dicom_id}.dcm\"\n",
        "    dicom_path = os.path.join(files_path, subject_prefix, subject_dir, study_dir, dicom_file)\n",
        "\n",
        "    if os.path.exists(dicom_path):\n",
        "        features = extract_features(dicom_path)\n",
        "        if features is not None:\n",
        "            densenet_vecs[dicom_id] = features\n",
        "\n",
        "    # Save intermediate results\n",
        "    if (idx + 1) % 500 == 0 or idx == len(train_df) - 1:\n",
        "        print(f\"Saving intermediate results: {len(densenet_vecs)} vectors\")\n",
        "        with open(os.path.join(output_dir, 'densenet121_train.pkl'), 'wb') as f:\n",
        "            pickle.dump(densenet_vecs, f)\n",
        "\n",
        "    # Clean up memory\n",
        "    if idx % 100 == 0:\n",
        "        gc.collect()\n",
        "\n",
        "# Save the final vectors\n",
        "with open(os.path.join(output_dir, 'densenet121_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(densenet_vecs, f)\n",
        "\n",
        "print(f\"Saved DenseNet features for {len(densenet_vecs)} train images\")\n",
        "\n",
        "# Process test images\n",
        "test_densenet_vecs = {}\n",
        "\n",
        "for idx, row in tqdm.tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test images\"):\n",
        "    if idx % batch_size == 0:\n",
        "        print(f\"Processed {idx}/{len(test_df)} test images\")\n",
        "\n",
        "    dicom_id = row['dicom_id']\n",
        "    subject_id = row['subject_id']\n",
        "    study_id = row['study_id']\n",
        "\n",
        "    # Construct path to the DICOM file\n",
        "    subject_prefix = f\"p{str(subject_id)[:2]}\"\n",
        "    subject_dir = f\"p{subject_id}\"\n",
        "    study_dir = f\"s{study_id}\"\n",
        "    dicom_file = f\"{dicom_id}.dcm\"\n",
        "    dicom_path = os.path.join(files_path, subject_prefix, subject_dir, study_dir, dicom_file)\n",
        "\n",
        "    if os.path.exists(dicom_path):\n",
        "        features = extract_features(dicom_path)\n",
        "        if features is not None:\n",
        "            test_densenet_vecs[dicom_id] = features\n",
        "\n",
        "    # Save intermediate results\n",
        "    if (idx + 1) % 100 == 0 or idx == len(test_df) - 1:\n",
        "        print(f\"Saving intermediate results: {len(test_densenet_vecs)} vectors\")\n",
        "        with open(os.path.join(output_dir, 'densenet121_test.pkl'), 'wb') as f:\n",
        "            pickle.dump(test_densenet_vecs, f)\n",
        "\n",
        "    # Clean up memory\n",
        "    if idx % 50 == 0:\n",
        "        gc.collect()\n",
        "\n",
        "# Save the test vectors\n",
        "with open(os.path.join(output_dir, 'densenet121_test.pkl'), 'wb') as f:\n",
        "    pickle.dump(test_densenet_vecs, f)\n",
        "\n",
        "print(f\"Saved DenseNet features for {len(test_densenet_vecs)} test images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMEgnN89zHQy",
        "outputId": "2ca3b514-b1f9-403d-d571-cb5aafadcff9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train data shape: (824, 3)\n",
            "Test data shape: (382, 3)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Loaded DenseNet121 model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing train images:   0%|          | 0/824 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:   4%|▎         | 29/824 [03:39<29:27,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/MyDrive/mimic-cxr-project/files/p14/p14187001/s54676352/f0d713e0-4db1800a-4ae3600b-c365a2a5-4985ec73.dcm: The number of bytes of pixel data is less than expected (1094284 vs 15548928 bytes) - the dataset may be corrupted, have an invalid group 0028 element value, or the transfer syntax may be incorrect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:   6%|▌         | 50/824 [04:57<34:05,  2.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 50/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  12%|█▏        | 100/824 [07:00<31:13,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  18%|█▊        | 150/824 [09:24<28:32,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 150/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  24%|██▍       | 200/824 [11:31<26:42,  2.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 200/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  30%|███       | 250/824 [13:36<20:43,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 250/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  36%|███▋      | 300/824 [15:32<17:01,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 300/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  42%|████▏     | 350/824 [17:35<19:15,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 350/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  49%|████▊     | 400/824 [19:37<15:20,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 400/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  55%|█████▍    | 450/824 [21:45<16:52,  2.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 450/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  61%|██████    | 500/824 [23:49<13:12,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intermediate results: 499 vectors\n",
            "Processed 500/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  67%|██████▋   | 550/824 [25:44<11:54,  2.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 550/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  73%|███████▎  | 600/824 [27:52<09:45,  2.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 600/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  76%|███████▌  | 628/824 [29:00<07:32,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/MyDrive/mimic-cxr-project/files/p15/p15065955/s57440443/097b23d1-80c1433e-72156058-d40e3101-d41cb853.dcm: The number of bytes of pixel data is less than expected (10384010 vs 15548928 bytes) - the dataset may be corrupted, have an invalid group 0028 element value, or the transfer syntax may be incorrect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  79%|███████▉  | 650/824 [29:50<06:30,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 650/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  85%|████████▍ | 700/824 [31:51<04:37,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 700/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  91%|█████████ | 750/824 [33:59<03:03,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 750/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images:  97%|█████████▋| 800/824 [36:01<00:56,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 800/824 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train images: 100%|██████████| 824/824 [36:55<00:00,  2.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intermediate results: 822 vectors\n",
            "Saved DenseNet features for 822 train images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test images:   0%|          | 0/382 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  12%|█▏        | 44/382 [01:45<09:37,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/MyDrive/mimic-cxr-project/files/p17/p17585916/s51774836/7f7346e9-c1f9639f-8e83f5bc-f166c421-69f3b162.dcm: The number of bytes of pixel data is less than expected (1274412 vs 15548928 bytes) - the dataset may be corrupted, have an invalid group 0028 element value, or the transfer syntax may be incorrect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  13%|█▎        | 50/382 [01:58<11:34,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 50/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  26%|██▌       | 100/382 [04:10<12:47,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intermediate results: 99 vectors\n",
            "Processed 100/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  39%|███▉      | 150/382 [06:16<10:31,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 150/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  52%|█████▏    | 200/382 [08:19<08:49,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intermediate results: 199 vectors\n",
            "Processed 200/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  65%|██████▌   | 250/382 [10:16<05:41,  2.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 250/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  79%|███████▊  | 300/382 [12:15<03:14,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intermediate results: 299 vectors\n",
            "Processed 300/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  90%|████████▉ | 343/382 [13:57<01:27,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing /content/drive/MyDrive/mimic-cxr-project/files/p16/p16741986/s56541794/fdba0667-faa73efd-da3746a5-2a72a1fa-f5b292b7.dcm: The number of bytes of pixel data is less than expected (3535498 vs 15548928 bytes) - the dataset may be corrupted, have an invalid group 0028 element value, or the transfer syntax may be incorrect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images:  92%|█████████▏| 350/382 [14:12<01:04,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 350/382 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test images: 100%|██████████| 382/382 [15:36<00:00,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving intermediate results: 380 vectors\n",
            "Saved DenseNet features for 380 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find nearest neighbors based on feature similarity\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the feature vectors if you're running this in a separate session\n",
        "train_features_path = os.path.join(output_dir, 'densenet121_train.pkl')\n",
        "test_features_path = os.path.join(output_dir, 'densenet121_test.pkl')\n",
        "\n",
        "with open(train_features_path, 'rb') as f:\n",
        "    densenet_vecs = pickle.load(f)\n",
        "\n",
        "with open(test_features_path, 'rb') as f:\n",
        "    test_densenet_vecs = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded {len(densenet_vecs)} train features and {len(test_densenet_vecs)} test features\")\n",
        "\n",
        "# Convert train features to numpy array for faster processing\n",
        "train_dicom_ids = list(densenet_vecs.keys())\n",
        "train_features = np.array([densenet_vecs[dicom_id] for dicom_id in train_dicom_ids])\n",
        "\n",
        "# Find top 100 nearest neighbors for each test image\n",
        "top100_neighbors = {}\n",
        "\n",
        "for test_dicom, test_features in tqdm.tqdm(test_densenet_vecs.items(), desc=\"Finding neighbors\"):\n",
        "    # Reshape test features for cosine similarity\n",
        "    test_features_reshaped = test_features.reshape(1, -1)\n",
        "\n",
        "    # Compute similarity to all training images\n",
        "    similarities = cosine_similarity(test_features_reshaped, train_features)[0]\n",
        "\n",
        "    # Get indices of top 100 most similar images\n",
        "    top_indices = similarities.argsort()[-100:][::-1]\n",
        "\n",
        "    # Get the corresponding dicom IDs\n",
        "    top_dicom_ids = [train_dicom_ids[i] for i in top_indices]\n",
        "\n",
        "    # Store in our neighbors dictionary\n",
        "    top100_neighbors[test_dicom] = top_dicom_ids\n",
        "\n",
        "# Save the neighbors\n",
        "neighbors_path = os.path.join(output_dir, 'top100_neighbors.pkl')\n",
        "with open(neighbors_path, 'wb') as f:\n",
        "    pickle.dump(top100_neighbors, f)\n",
        "\n",
        "print(f\"Saved top 100 neighbors for {len(top100_neighbors)} test images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wuyg5CirFaps",
        "outputId": "11eb19e7-b63f-4fd6-9fcf-7330fc5da0ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 822 train features and 380 test features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Finding neighbors: 100%|██████████| 380/380 [00:02<00:00, 134.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved top 100 neighbors for 380 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}