{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "V5E1",
   "authorship_tag": "ABX9TyNvZf9716USGnhbm/gJ7mBN",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jasleenkaursandhu/Reproducing-chest-xray-report-generation-boag/blob/main/3gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sP0BluOO0DY1",
    "ExecuteTime": {
     "end_time": "2025-04-25T17:44:14.155276Z",
     "start_time": "2025-04-25T17:34:44.930506Z"
    }
   },
   "source": [
    "# N-gram Model for Report Generation\n",
    "# This notebook implements a conditional n-gram language model for chest X-ray report generation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# Set up paths\n",
    "base_path = '/Users/simeon/Documents/DLH/content/mimic-cxr-project'\n",
    "# !mkdir -p {base_path}/data\n",
    "# !mkdir -p {base_path}/output\n",
    "\n",
    "# Import the report parser module\n",
    "import sys\n",
    "sys.path.append(f\"{base_path}/modules\")\n",
    "from report_parser import parse_report, MIMIC_RE\n",
    "print(\"Successfully imported report parser module\")\n",
    "\n",
    "# Load train and test data\n",
    "data_dir = os.path.join(base_path, 'data')\n",
    "files_path = os.path.join(base_path, 'new_files')\n",
    "output_dir = os.path.join(base_path, 'output')\n",
    "reports_dir = os.path.join(base_path, 'reports')\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t')\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.tsv'), sep='\\t')\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Load neighbors for different k values (10, 50, 100, 200)\n",
    "neighbors_dict = {}\n",
    "k_values = [10, 50, 100, 200]\n",
    "\n",
    "for k in k_values:\n",
    "    neighbors_path = os.path.join(output_dir, f'{k}nn_neighbors.pkl')\n",
    "\n",
    "    if os.path.exists(neighbors_path):\n",
    "        with open(neighbors_path, 'rb') as f:\n",
    "            neighbors = pickle.load(f)\n",
    "\n",
    "        neighbors_dict[k] = neighbors\n",
    "        print(f\"Loaded {k} neighbors for {len(neighbors)} test images\")\n",
    "    else:\n",
    "        print(f\"Warning: Neighbors file not found at {neighbors_path}\")\n",
    "\n",
    "# Map each dicom to its study_id\n",
    "report_id_column = 'study_id'\n",
    "report_lookup = dict(train_df[['dicom_id', report_id_column]].values)\n",
    "print(f\"Created lookup dictionary for {len(report_lookup)} training images\")\n",
    "\n",
    "# Define the n-gram model\n",
    "class ConditionalNGramLM:\n",
    "    \"\"\"\n",
    "    Conditional n-gram language model as described in the paper.\n",
    "\n",
    "    For each test image, we build a language model based on\n",
    "    the reports of its closest k training images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=3):\n",
    "        \"\"\"Initialize the n-gram model with specified n.\"\"\"\n",
    "        self.n = n\n",
    "        self.START = \"<s>\"\n",
    "        self.END = \"</s>\"\n",
    "\n",
    "    def build_lm(self, reports):\n",
    "        \"\"\"\n",
    "        Build an n-gram language model from a collection of reports.\n",
    "\n",
    "        Args:\n",
    "            reports (list): List of report texts\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary mapping n-gram contexts to next word distributions\n",
    "        \"\"\"\n",
    "        if not reports:\n",
    "            return {}\n",
    "\n",
    "        # Language model dictionary\n",
    "        lm = defaultdict(Counter)\n",
    "\n",
    "        for report in reports:\n",
    "            if not report or not isinstance(report, str):\n",
    "                continue\n",
    "\n",
    "            # Tokenize and preprocess\n",
    "            tokens = report.lower().split()\n",
    "\n",
    "            # Handle special case for unigram (1-gram) model\n",
    "            if self.n == 1:\n",
    "                # For 1-gram, we just need word frequencies (no context)\n",
    "                for token in tokens:\n",
    "                    lm[()][token] += 1\n",
    "                # Add END token with appropriate frequency\n",
    "                lm[()][self.END] += 1\n",
    "            else:\n",
    "                # Add START and END tokens\n",
    "                padded_tokens = [self.START] * (self.n - 1) + tokens + [self.END]\n",
    "\n",
    "                # Build n-grams\n",
    "                for i in range(len(padded_tokens) - self.n + 1):\n",
    "                    context = tuple(padded_tokens[i:i+self.n-1])\n",
    "                    next_word = padded_tokens[i+self.n-1]\n",
    "                    lm[context][next_word] += 1\n",
    "\n",
    "        return lm\n",
    "\n",
    "    def sample(self, lm):\n",
    "        \"\"\"\n",
    "        Generate text by sampling from the language model.\n",
    "\n",
    "        Args:\n",
    "            lm (dict): Language model\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text\n",
    "        \"\"\"\n",
    "        if not lm:\n",
    "            return \"\"\n",
    "\n",
    "        # Handle special case for unigram model\n",
    "        if self.n == 1:\n",
    "            if () not in lm:\n",
    "                return \"\"\n",
    "\n",
    "            # Generate sequence for unigram model\n",
    "            generated = []\n",
    "            max_length = 100  # Prevent infinite loops\n",
    "\n",
    "            # Sample words based on their frequency until END or max_length\n",
    "            while len(generated) < max_length:\n",
    "                # Get all words and their counts\n",
    "                words, counts = zip(*lm[()].items())\n",
    "                total = sum(counts)\n",
    "                probs = [count/total for count in counts]\n",
    "\n",
    "                # Sample a word\n",
    "                current_word = np.random.choice(words, p=probs)\n",
    "\n",
    "                # Stop if END token is sampled\n",
    "                if current_word == self.END:\n",
    "                    break\n",
    "\n",
    "                generated.append(current_word)\n",
    "\n",
    "            return \" \".join(generated)\n",
    "\n",
    "        # Standard n-gram model (n ≥ 2)\n",
    "        # Start with START tokens\n",
    "        generated = [self.START] * (self.n - 1)\n",
    "        current_word = self.START\n",
    "\n",
    "        # Generate words until END token or max length reached\n",
    "        max_length = 100  # Prevent infinite loops\n",
    "        while current_word != self.END and len(generated) < max_length:\n",
    "            # Get the current context\n",
    "            context = tuple(generated[-(self.n-1):])\n",
    "\n",
    "            # If context not in language model, stop generation\n",
    "            if context not in lm or not lm[context]:\n",
    "                break\n",
    "\n",
    "            # Sample from the distribution of next words\n",
    "            next_words = lm[context]\n",
    "            words, counts = zip(*next_words.items())\n",
    "            total = sum(counts)\n",
    "            probs = [count/total for count in counts]\n",
    "\n",
    "            current_word = np.random.choice(words, p=probs)\n",
    "            generated.append(current_word)\n",
    "\n",
    "        # Remove START tokens and END token if present\n",
    "        result = generated[(self.n-1):] if self.n > 1 else generated\n",
    "        if result and result[-1] == self.END:\n",
    "            result = result[:-1]\n",
    "\n",
    "        return \" \".join(result)\n",
    "\n",
    "    def generate_report(self, neighbor_reports):\n",
    "        \"\"\"\n",
    "        Generate a report for a test image based on its neighbors' reports.\n",
    "\n",
    "        Args:\n",
    "            neighbor_reports (list): Reports from neighboring training images\n",
    "\n",
    "        Returns:\n",
    "            str: Generated report\n",
    "        \"\"\"\n",
    "        # Build language model from neighbor reports\n",
    "        lm = self.build_lm(neighbor_reports)\n",
    "\n",
    "        # Sample from the language model\n",
    "        return self.sample(lm)\n",
    "\n",
    "# Function to retrieve reports for a list of DICOM IDs\n",
    "def get_reports_for_dicoms(dicom_ids):\n",
    "    \"\"\"\n",
    "    Get the reports for a list of DICOM IDs.\n",
    "\n",
    "    Args:\n",
    "        dicom_ids (list): List of DICOM IDs\n",
    "\n",
    "    Returns:\n",
    "        list: List of report texts\n",
    "    \"\"\"\n",
    "    reports = []\n",
    "\n",
    "    for dicom_id in dicom_ids:\n",
    "        # Skip if no report lookup available\n",
    "        if dicom_id not in report_lookup:\n",
    "            continue\n",
    "\n",
    "        # Get report ID and subject ID\n",
    "        report_id = report_lookup[dicom_id]\n",
    "        subject_row = train_df[train_df.dicom_id == dicom_id]\n",
    "\n",
    "        if len(subject_row) == 0:\n",
    "            continue\n",
    "\n",
    "        subject_id = subject_row.iloc[0]['subject_id']\n",
    "\n",
    "        # Construct path to report\n",
    "        subject_prefix = f\"p{str(subject_id)[:2]}\"\n",
    "        subject_dir = f\"p{subject_id}\"\n",
    "        study_dir = f\"s{report_id}\"\n",
    "        report_path = os.path.join(reports_dir, 'files', subject_prefix, subject_dir, f\"{study_dir}.txt\")\n",
    "\n",
    "        # Parse report\n",
    "        try:\n",
    "            if os.path.exists(report_path):\n",
    "                report = parse_report(report_path)\n",
    "\n",
    "                # Add findings section if available\n",
    "                if 'findings' in report:\n",
    "                    reports.append(report['findings'])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return reports\n",
    "\n",
    "# Generate reports using different n-gram sizes and different numbers of neighbors\n",
    "for n_value in [1, 2, 3]:  # The paper tested 1-gram, 2-gram, and 3-gram models\n",
    "    for k in k_values:  # Different numbers of neighbors\n",
    "        print(f\"\\nGenerating reports with {n_value}-gram model using {k} nearest neighbors...\")\n",
    "\n",
    "        # Skip if this k value is not available\n",
    "        if k not in neighbors_dict:\n",
    "            print(f\"Skipping {k} neighbors as the data is not available\")\n",
    "            continue\n",
    "\n",
    "        # Get neighbors for this k value\n",
    "        neighbors = neighbors_dict[k]\n",
    "\n",
    "        # Initialize n-gram model\n",
    "        ngram_model = ConditionalNGramLM(n=n_value)\n",
    "\n",
    "        # Generate reports for test images\n",
    "        generated_reports = {}\n",
    "\n",
    "        for pred_dicom in tqdm.tqdm(test_df.dicom_id.values):\n",
    "            # Skip if no neighbors\n",
    "            if pred_dicom not in neighbors:\n",
    "                print(f\"Warning: No neighbors for {pred_dicom}\")\n",
    "                continue\n",
    "\n",
    "            # Get closest k training images\n",
    "            nn_dicoms = neighbors[pred_dicom][:k]  # Ensure we use only k neighbors\n",
    "\n",
    "            # Get reports for these neighbors\n",
    "            neighbor_reports = get_reports_for_dicoms(nn_dicoms)\n",
    "\n",
    "            # Skip if no reports found\n",
    "            if not neighbor_reports:\n",
    "                continue\n",
    "\n",
    "            # Generate report\n",
    "            generated_text = ngram_model.generate_report(neighbor_reports)\n",
    "            generated_reports[pred_dicom] = generated_text\n",
    "\n",
    "        print(f\"Generated reports for {len(generated_reports)}/{len(test_df)} test images\")\n",
    "\n",
    "        # Save the generated reports\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "\n",
    "        pred_file = os.path.join(output_dir, f'{n_value}-gram_{k}nn.tsv')\n",
    "        print(f\"Saving predictions to {pred_file}\")\n",
    "\n",
    "        with open(pred_file, 'w') as f:\n",
    "            print('dicom_id\\tgenerated', file=f)\n",
    "            for dicom_id, generated in sorted(generated_reports.items()):\n",
    "                # Clean up the text (remove any tabs)\n",
    "                cleaned_text = generated.replace('\\t', ' ')\n",
    "                print(f'{dicom_id}\\t{cleaned_text}', file=f)\n",
    "\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "\n",
    "        # Display sample reports for 3-gram model with each k value\n",
    "        if n_value == 3:\n",
    "            print(f\"\\nSample reports from 3-gram model with {k} nearest neighbors:\")\n",
    "            sample_count = min(3, len(generated_reports))\n",
    "            sample_dicoms = list(generated_reports.keys())[:sample_count]\n",
    "\n",
    "            for dicom_id in sample_dicoms:\n",
    "                print(f\"\\nSample report for {dicom_id}:\")\n",
    "                report_text = generated_reports[dicom_id]\n",
    "\n",
    "                # Print preview of the report\n",
    "                if len(report_text) > 200:\n",
    "                    print(report_text[:200] + \"...\")\n",
    "                else:\n",
    "                    print(report_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported report parser module\n",
      "Train data shape: (4291, 3)\n",
      "Test data shape: (1757, 3)\n",
      "Loaded 10 neighbors for 1757 test images\n",
      "Loaded 50 neighbors for 1757 test images\n",
      "Loaded 100 neighbors for 1757 test images\n",
      "Loaded 200 neighbors for 1757 test images\n",
      "Created lookup dictionary for 4291 training images\n",
      "\n",
      "Generating reports with 1-gram model using 10 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:08<00:00, 202.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:34:54\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/1-gram_10nn.tsv\n",
      "2025-04-25 17:34:54\n",
      "\n",
      "Generating reports with 1-gram model using 50 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:30<00:00, 57.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:35:25\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/1-gram_50nn.tsv\n",
      "2025-04-25 17:35:25\n",
      "\n",
      "Generating reports with 1-gram model using 100 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [01:03<00:00, 27.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:36:29\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/1-gram_100nn.tsv\n",
      "2025-04-25 17:36:29\n",
      "\n",
      "Generating reports with 1-gram model using 200 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [02:01<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:38:30\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/1-gram_200nn.tsv\n",
      "2025-04-25 17:38:30\n",
      "\n",
      "Generating reports with 2-gram model using 10 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:05<00:00, 331.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:38:35\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/2-gram_10nn.tsv\n",
      "2025-04-25 17:38:35\n",
      "\n",
      "Generating reports with 2-gram model using 50 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:24<00:00, 72.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:38:59\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/2-gram_50nn.tsv\n",
      "2025-04-25 17:38:59\n",
      "\n",
      "Generating reports with 2-gram model using 100 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:47<00:00, 36.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:39:47\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/2-gram_100nn.tsv\n",
      "2025-04-25 17:39:47\n",
      "\n",
      "Generating reports with 2-gram model using 200 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [01:32<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:41:19\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/2-gram_200nn.tsv\n",
      "2025-04-25 17:41:19\n",
      "\n",
      "Generating reports with 3-gram model using 10 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:05<00:00, 335.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:41:24\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/3-gram_10nn.tsv\n",
      "2025-04-25 17:41:24\n",
      "\n",
      "Sample reports from 3-gram model with 10 nearest neighbors:\n",
      "\n",
      "Sample report for 20386a2d-1f7a8868-f12e22ac-0d625d27-4c38c8e2:\n",
      "heart size is normal. peribronchial opacities in the region of the left pleural drainage catheter. the left pleural effusion appears minimally decreased in size with decreased, adjacent compressive at...\n",
      "\n",
      "Sample report for 63100eab-9e8a8d90-392bc822-325de482-69a64e3b:\n",
      "portable ap upright chest film at time is submitted\n",
      "\n",
      "Sample report for 17269efa-b016a94d-1361e8df-ac428071-d1133672:\n",
      "as compared to prior there continues to be volume loss at both bases. there is no pneumothorax. the cardiomediastinal and hilar contours are unremarkable. mild interstitial prominence with peribronchi...\n",
      "\n",
      "Generating reports with 3-gram model using 50 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:24<00:00, 71.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:41:49\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/3-gram_50nn.tsv\n",
      "2025-04-25 17:41:49\n",
      "\n",
      "Sample reports from 3-gram model with 50 nearest neighbors:\n",
      "\n",
      "Sample report for 20386a2d-1f7a8868-f12e22ac-0d625d27-4c38c8e2:\n",
      "there is no pleural effusion. chronic posttraumatic change right posterior fifth rib is stable.\n",
      "\n",
      "Sample report for 63100eab-9e8a8d90-392bc822-325de482-69a64e3b:\n",
      "portable ap upright chest radiograph at time\n",
      "\n",
      "Sample report for 17269efa-b016a94d-1361e8df-ac428071-d1133672:\n",
      "compared the prior study there is redemonstration of free intra-abdominal air, likely related to prior chest radiograph from , lung volumes are slightly reduced. the heart is normal in size.\n",
      "\n",
      "Generating reports with 3-gram model using 100 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [00:48<00:00, 36.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:42:37\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/3-gram_100nn.tsv\n",
      "2025-04-25 17:42:37\n",
      "\n",
      "Sample reports from 3-gram model with 100 nearest neighbors:\n",
      "\n",
      "Sample report for 20386a2d-1f7a8868-f12e22ac-0d625d27-4c38c8e2:\n",
      "there is some increased perihilar fullness, particularly on the prior study. there is no pleural effusion or pneumothorax. visualized osseous structures demonstrates no acute osseous abnormalities.\n",
      "\n",
      "Sample report for 63100eab-9e8a8d90-392bc822-325de482-69a64e3b:\n",
      "elevation of left hemidiaphragm. cardiomediastinal silhouette is normal. the hilar and mediastinal contours are unchanged. no free air below the right costophrenic sulcus is unchanged. the right lung ...\n",
      "\n",
      "Sample report for 17269efa-b016a94d-1361e8df-ac428071-d1133672:\n",
      "trace layering pleural effusions are identified. there is no significant interval change.\n",
      "\n",
      "Generating reports with 3-gram model using 200 nearest neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1757/1757 [01:36<00:00, 18.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated reports for 1757/1757 test images\n",
      "2025-04-25 17:44:14\n",
      "Saving predictions to /Users/simeon/Documents/DLH/content/mimic-cxr-project/output/3-gram_200nn.tsv\n",
      "2025-04-25 17:44:14\n",
      "\n",
      "Sample reports from 3-gram model with 200 nearest neighbors:\n",
      "\n",
      "Sample report for 20386a2d-1f7a8868-f12e22ac-0d625d27-4c38c8e2:\n",
      "tiny left pneumothorax is seen. staple lines project over the prevascular region of the costophrenic angles are not included. there is a pleural line projecting over the lung bases likely reflect edem...\n",
      "\n",
      "Sample report for 63100eab-9e8a8d90-392bc822-325de482-69a64e3b:\n",
      "the heart is normal. imaged osseous structures are unremarkable in appearance.\n",
      "\n",
      "Sample report for 17269efa-b016a94d-1361e8df-ac428071-d1133672:\n",
      "patchy, streaky opacities in both lung fields suggest subsegmental atelectasis is noted in the lower cervical spine. et tube with tip in the left lung volume. possible small left and small pleural flu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ]
}
