{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CNN-RNN + Beam Search Model for Chest X-Ray Report Generation\n",
    "# Based on the Boag et al. paper \"Baselines for Chest X-Ray Report Generation\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import time\n",
    "import tqdm\n",
    "import pydicom\n",
    "import string\n",
    "import certifi\n",
    "import ssl\n",
    "\n",
    "# Fix SSL certificate issues in macOS\n",
    "import os\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "# Simple tokenizer to avoid NLTK dependency\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple tokenizer that splits text on whitespace and punctuation.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Remove punctuation and replace with space\n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "\n",
    "    # Split on whitespace and filter empty tokens\n",
    "    tokens = [token for token in text.lower().split() if token.strip()]\n",
    "    return tokens\n",
    "\n",
    "# Define paths\n",
    "base_path = '/Users/simeon/Documents/DLH/content/mimic-cxr-project'\n",
    "data_dir = os.path.join(base_path, 'data')\n",
    "files_path = os.path.join(base_path, 'files')\n",
    "output_dir = os.path.join(base_path, 'output')\n",
    "reports_dir = os.path.join(base_path, 'reports')\n",
    "models_dir = os.path.join(base_path, 'models')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Import the report parser module\n",
    "import sys\n",
    "sys.path.append(f\"{base_path}/modules\")\n",
    "from report_parser import parse_report, MIMIC_RE\n",
    "print(\"Successfully imported report parser module\")\n",
    "\n",
    "# Load train and test data\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t')\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.tsv'), sep='\\t')\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Vocabulary class to handle tokenization\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            # Use simple tokenizer\n",
    "            for word in simple_tokenize(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "\n",
    "                if word not in self.stoi and frequencies[word] >= self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        # Use simple tokenizer\n",
    "        tokenized_text = simple_tokenize(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n",
    "# Class to load and preprocess the data\n",
    "class ChestXRayReportDataset:\n",
    "    def __init__(self, df, is_train=True, transform=None, max_seq_length=100):\n",
    "        self.df = df\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.reports = []\n",
    "\n",
    "        # Extract reports if training\n",
    "        if self.is_train:\n",
    "            print(\"Extracting report texts for training data...\")\n",
    "            for _, row in tqdm.tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "                subject_id = row['subject_id']\n",
    "                study_id = row['study_id']\n",
    "\n",
    "                # Construct path to report\n",
    "                subject_prefix = f\"p{str(subject_id)[:2]}\"\n",
    "                subject_dir = f\"p{subject_id}\"\n",
    "                study_dir = f\"s{study_id}\"\n",
    "                report_path = os.path.join(reports_dir, 'files', subject_prefix, subject_dir, f\"{study_dir}.txt\")\n",
    "\n",
    "                try:\n",
    "                    if os.path.exists(report_path):\n",
    "                        report = parse_report(report_path)\n",
    "                        if 'findings' in report and report['findings']:\n",
    "                            self.reports.append((row['dicom_id'], report['findings']))\n",
    "                except Exception as e:\n",
    "                    pass  # Skip reports that can't be parsed\n",
    "\n",
    "            print(f\"Extracted {len(self.reports)} reports from training data\")\n",
    "\n",
    "        self.dicom_paths = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            subject_id = row['subject_id']\n",
    "            study_id = row['study_id']\n",
    "            dicom_id = row['dicom_id']\n",
    "\n",
    "            # Construct path to DICOM file\n",
    "            subject_prefix = f\"p{str(subject_id)[:2]}\"\n",
    "            subject_dir = f\"p{subject_id}\"\n",
    "            study_dir = f\"s{study_id}\"\n",
    "            dicom_file = f\"{dicom_id}.dcm\"\n",
    "            dicom_path = os.path.join(files_path, subject_prefix, subject_dir, study_dir, dicom_file)\n",
    "\n",
    "            if os.path.exists(dicom_path):\n",
    "                self.dicom_paths.append((dicom_id, dicom_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dicom_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dicom_id, dicom_path = self.dicom_paths[idx]\n",
    "\n",
    "        # Load and transform image\n",
    "        try:\n",
    "            ds = pydicom.dcmread(dicom_path)\n",
    "            pixel_array = ds.pixel_array\n",
    "\n",
    "            # Normalize and convert to RGB\n",
    "            pixel_array = pixel_array / np.max(pixel_array)\n",
    "            img = np.uint8(pixel_array * 255)\n",
    "\n",
    "            # Convert to RGB\n",
    "            if len(img.shape) == 2:\n",
    "                img_rgb = np.stack([img, img, img], axis=2)\n",
    "            elif img.shape[2] == 1:\n",
    "                img_rgb = np.concatenate([img, img, img], axis=2)\n",
    "            else:\n",
    "                img_rgb = img\n",
    "\n",
    "            pil_img = Image.fromarray(img_rgb)\n",
    "            if self.transform:\n",
    "                image = self.transform(pil_img)\n",
    "        except Exception as e:\n",
    "            # Create blank image if loading fails\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "\n",
    "        # Return image and report for training, only image for testing\n",
    "        if self.is_train:\n",
    "            for report_id, report_text in self.reports:\n",
    "                if report_id == dicom_id:\n",
    "                    return image, report_text, dicom_id\n",
    "            # Return empty report if none found\n",
    "            return image, \"\", dicom_id\n",
    "        else:\n",
    "            return image, dicom_id\n",
    "\n",
    "# Create a simplified CNN-RNN model that follows the paper\n",
    "class CNNRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, cnn_feature_size):\n",
    "        super(CNNRNNModel, self).__init__()\n",
    "\n",
    "        # CNN encoder (DenseNet121)\n",
    "        print(\"Loading DenseNet121 model without pretrained weights...\")\n",
    "        self.densenet = models.densenet121(pretrained=False)\n",
    "        self.densenet.classifier = nn.Linear(cnn_feature_size, embed_size)\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # LSTM decoder\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images, captions=None, teacher_forcing_ratio=0.5):\n",
    "        # Extract features from images using CNN encoder\n",
    "        # [batch_size, 3, 224, 224] -> [batch_size, embed_size]\n",
    "        features = self.densenet(images)\n",
    "\n",
    "        # Training mode with captions\n",
    "        if captions is not None:\n",
    "            batch_size = features.size(0)\n",
    "            caption_length = captions.size(1)\n",
    "\n",
    "            # Initialize tensor for storing outputs\n",
    "            outputs = torch.zeros(batch_size, caption_length, self.fc.out_features).to(device)\n",
    "\n",
    "            # Initialize hidden and cell state\n",
    "            h = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)\n",
    "            c = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)\n",
    "\n",
    "            # First input to LSTM is the image features\n",
    "            # [batch_size, embed_size] -> [batch_size, 1, embed_size]\n",
    "            x = features.unsqueeze(1)\n",
    "\n",
    "            # Generate caption one word at a time\n",
    "            for t in range(caption_length):\n",
    "                # Forward through LSTM\n",
    "                lstm_out, (h, c) = self.lstm(x, (h, c))\n",
    "\n",
    "                # Forward through fully connected layer\n",
    "                out = self.fc(lstm_out)\n",
    "\n",
    "                # Store output\n",
    "                outputs[:, t] = out.squeeze(1)\n",
    "\n",
    "                # Teacher forcing\n",
    "                use_teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "\n",
    "                if use_teacher_force and t < caption_length - 1:\n",
    "                    # Use ground truth as next input\n",
    "                    # [batch_size, 1] -> [batch_size, 1, embed_size]\n",
    "                    x = self.embedding(captions[:, t+1].unsqueeze(1))\n",
    "                else:\n",
    "                    # Use predicted word as next input\n",
    "                    # [batch_size, 1, vocab_size] -> [batch_size, 1, 1]\n",
    "                    pred_token = out.argmax(2)\n",
    "                    # [batch_size, 1, 1] -> [batch_size, 1, embed_size]\n",
    "                    x = self.embedding(pred_token)\n",
    "\n",
    "            return outputs\n",
    "\n",
    "        # Inference mode\n",
    "        else:\n",
    "            batch_size = features.size(0)\n",
    "\n",
    "            # Start with <SOS> token (index 1)\n",
    "            current_token = torch.ones(batch_size, 1, dtype=torch.long).to(device)\n",
    "\n",
    "            # Initialize hidden and cell state\n",
    "            h = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)\n",
    "            c = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)\n",
    "\n",
    "            # First input to LSTM is the image features\n",
    "            x = features.unsqueeze(1)\n",
    "\n",
    "            # Store generated captions\n",
    "            outputs = []\n",
    "\n",
    "            # Generate first token using image features\n",
    "            lstm_out, (h, c) = self.lstm(x, (h, c))\n",
    "            out = self.fc(lstm_out)\n",
    "            pred_token = out.argmax(2)\n",
    "            outputs.append(pred_token.squeeze(1))\n",
    "\n",
    "            # Use predicted token as input for next step\n",
    "            x = self.embedding(pred_token)\n",
    "\n",
    "            # Maximum length is 100 tokens\n",
    "            for _ in range(99):  # Already generated first token\n",
    "                # Forward through LSTM\n",
    "                lstm_out, (h, c) = self.lstm(x, (h, c))\n",
    "\n",
    "                # Forward through fully connected layer\n",
    "                out = self.fc(lstm_out)\n",
    "\n",
    "                # Get predicted token\n",
    "                pred_token = out.argmax(2)\n",
    "\n",
    "                # Add to outputs\n",
    "                outputs.append(pred_token.squeeze(1))\n",
    "\n",
    "                # Break if <EOS> token (index 2) is predicted\n",
    "                if (pred_token == 2).all():\n",
    "                    break\n",
    "\n",
    "                # Prepare next input\n",
    "                x = self.embedding(pred_token)\n",
    "\n",
    "            # Convert list of tensors to tensor\n",
    "            return torch.stack(outputs, dim=1)\n",
    "\n",
    "# Beam Search implementation\n",
    "class BeamSearch:\n",
    "    def __init__(self, model, beam_size=4, max_length=100):\n",
    "        self.model = model\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    # Modify the beam search to include a repetition penalty\n",
    "def search(self, image, vocab):\n",
    "    \"\"\"\n",
    "    Perform beam search to generate caption with repetition penalty\n",
    "\n",
    "    Args:\n",
    "        image: Input image tensor [1, 3, 224, 224]\n",
    "        vocab: Vocabulary object for mapping indices to words\n",
    "\n",
    "    Returns:\n",
    "        string: Generated caption\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Extract features from image\n",
    "        features = self.model.densenet(image)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        batch_size = 1\n",
    "        h = torch.zeros(1, batch_size, self.model.lstm.hidden_size).to(device)\n",
    "        c = torch.zeros(1, batch_size, self.model.lstm.hidden_size).to(device)\n",
    "\n",
    "        # First input to LSTM is the image features\n",
    "        x = features.unsqueeze(1)\n",
    "\n",
    "        # Forward pass to get first predictions\n",
    "        lstm_out, (h, c) = self.model.lstm(x, (h, c))\n",
    "        outputs = self.model.fc(lstm_out)\n",
    "\n",
    "        # Get top beam_size predictions\n",
    "        log_probs, indexes = torch.topk(F.log_softmax(outputs.squeeze(1), dim=1), self.beam_size)\n",
    "\n",
    "        # Initialize beams\n",
    "        beams = [(log_probs[0][i].item(), [indexes[0][i].item()], h, c) for i in range(self.beam_size)]\n",
    "\n",
    "        # Generate remaining words\n",
    "        for _ in range(self.max_length - 1):\n",
    "            # Collect new candidates\n",
    "            candidates = []\n",
    "\n",
    "            # Expand each beam\n",
    "            for log_prob, seq, hidden_h, hidden_c in beams:\n",
    "                # If sequence ends with <EOS>, keep it as is\n",
    "                if seq[-1] == 2:  # <EOS> token\n",
    "                    candidates.append((log_prob, seq, hidden_h, hidden_c))\n",
    "                    continue\n",
    "\n",
    "                # Prepare input\n",
    "                word = torch.tensor([[seq[-1]]], dtype=torch.long).to(device)\n",
    "                x = self.model.embedding(word)\n",
    "\n",
    "                # Forward pass\n",
    "                lstm_out, (h_new, c_new) = self.model.lstm(x, (hidden_h, hidden_c))\n",
    "                outputs = self.model.fc(lstm_out)\n",
    "\n",
    "                # Get log probabilities with repetition penalty\n",
    "                logits = outputs.squeeze(1)\n",
    "\n",
    "                # Apply repetition penalty - reduce probability of tokens that have already appeared\n",
    "                if len(seq) > 1:  # Only apply after first token\n",
    "                    for prev_token in set(seq):\n",
    "                        logits[0, prev_token] /= 1.5  # Penalty factor\n",
    "\n",
    "                log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "                # Get top k predictions\n",
    "                word_log_probs, word_indices = torch.topk(log_probs, self.beam_size)\n",
    "\n",
    "                # Add new candidates\n",
    "                for i in range(self.beam_size):\n",
    "                    # Check for repetitions of 3 or more consecutive tokens\n",
    "                    candidate_token = word_indices[0][i].item()\n",
    "                    if len(seq) >= 2 and seq[-1] == seq[-2] == candidate_token:\n",
    "                        continue  # Skip this candidate to avoid repetition\n",
    "\n",
    "                    candidate_log_prob = log_prob + word_log_probs[0][i].item()\n",
    "                    candidate_seq = seq + [candidate_token]\n",
    "                    candidates.append((candidate_log_prob, candidate_seq, h_new, c_new))\n",
    "\n",
    "            # If no candidates (all beams hit repetition limits), break\n",
    "            if not candidates:\n",
    "                break\n",
    "\n",
    "            # Keep top beam_size candidates\n",
    "            candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            beams = candidates[:self.beam_size]\n",
    "\n",
    "            # Stop if all beams end with <EOS> or max length reached\n",
    "            if all(beam[1][-1] == 2 for beam in beams):\n",
    "                break\n",
    "\n",
    "        # Get best beam, normalize by length for longer sequences\n",
    "        best_beam = max(beams, key=lambda x: x[0] / len(x[1]) if len(x[1]) > 10 else x[0])\n",
    "        best_seq = best_beam[1]\n",
    "\n",
    "        # Convert to words\n",
    "        caption = []\n",
    "        for token_id in best_seq:\n",
    "            if token_id == 2:  # <EOS>\n",
    "                break\n",
    "            if token_id > 3:  # Skip <PAD>, < SOS >, <EOS>, <UNK>\n",
    "                caption.append(vocab.itos[token_id])\n",
    "\n",
    "        # If caption is too short, use another beam\n",
    "        if len(caption) < 5 and len(beams) > 1:\n",
    "            beams.sort(key=lambda x: x[0] / len(x[1]) if len(x[1]) > 10 else x[0], reverse=True)\n",
    "            for beam in beams[1:]:  # Try other beams\n",
    "                alt_seq = beam[1]\n",
    "                alt_caption = []\n",
    "                for token_id in alt_seq:\n",
    "                    if token_id == 2:  # <EOS>\n",
    "                        break\n",
    "                    if token_id > 3:  # Skip <PAD>, < SOS >, <EOS>, <UNK>\n",
    "                        alt_caption.append(vocab.itos[token_id])\n",
    "                if len(alt_caption) >= 5:\n",
    "                    return \" \".join(alt_caption)\n",
    "\n",
    "        return \" \".join(caption)\n",
    "\n",
    "# Function to generate dummy data for testing the model\n",
    "def create_dummy_batch(batch_size=2, seq_len=10, vocab_size=100):\n",
    "    # Create dummy images\n",
    "    images = torch.randn(batch_size, 3, 224, 224)\n",
    "\n",
    "    # Create dummy captions\n",
    "    captions = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    return images, captions\n",
    "\n",
    "# Main execution function\n",
    "def run():\n",
    "    # Create dataset for training\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_dataset = ChestXRayReportDataset(train_df, is_train=True, transform=transform)\n",
    "\n",
    "    # Build vocabulary\n",
    "    print(\"Building vocabulary...\")\n",
    "    vocab = Vocabulary()\n",
    "    all_reports = [report for _, report, _ in train_dataset]\n",
    "    vocab.build_vocabulary(all_reports)\n",
    "    print(f\"Built vocabulary with {len(vocab)} tokens\")\n",
    "\n",
    "    # Save vocabulary\n",
    "    vocab_path = os.path.join(models_dir, 'vocab.pkl')\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0  # Set to 0 to avoid multiprocessing issues on Mac\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNNRNNModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_size=256,\n",
    "        hidden_size=512,\n",
    "        cnn_feature_size=1024\n",
    "    )\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Test model with dummy data to verify dimensions\n",
    "    print(\"Testing model with dummy data...\")\n",
    "    dummy_images, dummy_captions = create_dummy_batch(batch_size=2, seq_len=10, vocab_size=len(vocab))\n",
    "    dummy_images = dummy_images.to(device)\n",
    "    dummy_captions = dummy_captions.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(dummy_images, dummy_captions)\n",
    "            print(f\"Dummy forward pass successful! Output shape: {outputs.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in dummy forward pass: {e}\")\n",
    "        return\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=16, gamma=0.5)  # LR decay every 16 epochs\n",
    "\n",
    "    # Load model if available, train it for just 1 epoch to see if it works\n",
    "    model_path = os.path.join(models_dir, 'cnn_rnn_beam.pth')\n",
    "    # Change this section in the run() function\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    else:\n",
    "        print(\"Training model for 64 epochs...\")\n",
    "        # Training loop for full 64 epochs\n",
    "        num_epochs = 64  # As specified in the paper\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            # Decrease teacher forcing ratio as training progresses\n",
    "            # \"We increase the probability of feeding a sample of the inferred probability\n",
    "            # to itself by 0.05 per 16 epochs\"\n",
    "            teacher_forcing_ratio = max(0.5 - (epoch // 16) * 0.05, 0.0)\n",
    "\n",
    "            for i, (images, captions, _) in enumerate(tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "                images = images.to(device)\n",
    "\n",
    "                # Tokenize and pad captions\n",
    "                tokenized_captions = []\n",
    "                for caption in captions:\n",
    "                    if caption:\n",
    "                        tokens = [1]  # < SOS >\n",
    "                        tokens.extend(vocab.numericalize(caption))\n",
    "                        tokens.append(2)  # <EOS>\n",
    "                    else:\n",
    "                        tokens = [1, 2]  # < SOS >, <EOS>\n",
    "                    tokenized_captions.append(tokens)\n",
    "\n",
    "                # Pad sequences\n",
    "                padded_captions = []\n",
    "                for tokens in tokenized_captions:\n",
    "                    if len(tokens) > 100:\n",
    "                        padded_captions.append(tokens[:100])\n",
    "                    else:\n",
    "                        padded_captions.append(tokens + [0] * (100 - len(tokens)))\n",
    "\n",
    "                captions_tensor = torch.tensor(padded_captions).to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images, captions_tensor, teacher_forcing_ratio)\n",
    "\n",
    "                # Reshape for loss calculation\n",
    "                outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "                targets = captions_tensor.reshape(-1)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update training loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Print progress\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Calculate average loss for the epoch\n",
    "            avg_loss = train_loss / len(train_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Teacher forcing: {teacher_forcing_ratio:.2f}\")\n",
    "\n",
    "            # Save model if it's the best so far\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Model saved with improved loss: {best_loss:.4f}\")\n",
    "\n",
    "    # Create test dataset\n",
    "    print(\"Creating test dataset...\")\n",
    "    test_dataset = ChestXRayReportDataset(test_df, is_train=False, transform=transform)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,  # Process one at a time for beam search\n",
    "        shuffle=False,\n",
    "        num_workers=0  # Set to 0 to avoid multiprocessing issues on Mac\n",
    "    )\n",
    "\n",
    "    # Generate reports for a small sample\n",
    "    print(\"Generating reports with beam search...\")\n",
    "    model.eval()\n",
    "    beam_search = BeamSearch(model, beam_size=4)  # Beam size 4 as in the paper\n",
    "\n",
    "    generated_reports = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Just process a few samples for demonstration\n",
    "        max_samples = 3\n",
    "        sample_count = 0\n",
    "\n",
    "        for images, dicom_ids in tqdm.tqdm(test_loader):\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "            images = images.to(device)\n",
    "            dicom_id = dicom_ids[0]  # Batch size is 1\n",
    "\n",
    "            try:\n",
    "                caption = beam_search.search(images, vocab)\n",
    "                generated_reports[dicom_id] = caption\n",
    "                sample_count += 1\n",
    "                print(f\"Generated report {sample_count}/{max_samples}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating report for {dicom_id}: {e}\")\n",
    "\n",
    "    # Show sample reports\n",
    "    print(\"\\nSample generated reports:\")\n",
    "    for dicom_id, report in generated_reports.items():\n",
    "        print(f\"\\nDICOM ID: {dicom_id}\")\n",
    "        print(f\"Report: {report}\")\n",
    "\n",
    "    # Save generated reports\n",
    "    if generated_reports:\n",
    "        report_df = pd.DataFrame({\n",
    "            'dicom_id': list(generated_reports.keys()),\n",
    "            'generated': list(generated_reports.values())\n",
    "        })\n",
    "\n",
    "        output_file = os.path.join(output_dir, 'cnn_rnn_beam_sample.tsv')\n",
    "        report_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "        print(f\"Generated {len(generated_reports)} reports and saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No reports were generated\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ],
   "id": "2bc4a40ea6ef0bf9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 9
}
